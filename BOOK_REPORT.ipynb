{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466204de",
   "metadata": {},
   "source": [
    "# The Lazy Book Report\n",
    "\n",
    "Your professor has assigned a book report on \"The Red-Headed League\" by Arthur Conan Doyle. \n",
    "\n",
    "You haven't read the book. And out of stubbornness, you won't.\n",
    "\n",
    "But you *have* learned NLP. Let's use it to answer the professor's questions without reading.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's fetch the text from Project Gutenberg and prepare it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a46884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story loaded: 4000 words in 3 sections\n",
      "Section sizes: [1333, 1333, 1334]\n"
     ]
    }
   ],
   "source": [
    "# Fetch and prepare text - RUN THIS CELL FIRST\n",
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "url = 'https://www.gutenberg.org/files/1661/1661-0.txt'\n",
    "req = urllib.request.Request(url, headers={'User-Agent': 'Python-urllib'})\n",
    "with urllib.request.urlopen(req, timeout=30) as resp:\n",
    "    text = resp.read().decode('utf-8')\n",
    "\n",
    "# Strip Gutenberg boilerplate\n",
    "text = text.split('*** START OF')[1].split('***')[1]\n",
    "text = text.split('*** END OF')[0]\n",
    "\n",
    "# Extract \"The Red-Headed League\" story (it's the second story in the collection)\n",
    "matches = list(re.finditer(r'THE RED-HEADED LEAGUE', text, re.IGNORECASE))\n",
    "story_start = matches[1].end()\n",
    "story_text = text[story_start:]\n",
    "story_end = re.search(r'\\n\\s*III\\.\\s*\\n', story_text)\n",
    "story_text = story_text[:story_end.start()] if story_end else story_text\n",
    "\n",
    "# Split into 3 sections by word count\n",
    "words = story_text.split()[:4000]\n",
    "section_size = len(words) // 3\n",
    "sections = [\n",
    "    ' '.join(words[:section_size]),\n",
    "    ' '.join(words[section_size:2*section_size]),\n",
    "    ' '.join(words[2*section_size:])\n",
    "]\n",
    "\n",
    "print(f\"Story loaded: {len(words)} words in {len(sections)} sections\")\n",
    "print(f\"Section sizes: {[len(s.split()) for s in sections]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e757a",
   "metadata": {},
   "source": [
    "## Professor's Questions\n",
    "\n",
    "Your professor wants you to answer 5 questions about the story. Let's use NLP to find the answers.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 1: Writing Style\n",
    "\n",
    "> \"This text is from the 1890s. What makes it different from modern writing?\"\n",
    "\n",
    "**NLP Method:** Use preprocessing to compute text statistics. Tokenize the text and calculate:\n",
    "- Vocabulary richness (unique words / total words)\n",
    "- Average sentence length\n",
    "- Average word length\n",
    "\n",
    "**Hint:** Formal, literary writing typically shows higher vocabulary richness and longer sentences than modern casual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1710e297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for 'The Red-Headed League':\n",
      "Vocabulary Richness: 0.10\n",
      "Average Sentence Length: 14.84 words\n",
      "Average Word Length: 4.19 characters\n"
     ]
    }
   ],
   "source": [
    "# Your code here: compute text statistics\n",
    "# You'll need: import string, import re\n",
    "# - Tokenize: remove punctuation, lowercase\n",
    "# - Sentences: split on sentence-ending punctuation\n",
    "# Calculate vocab_richness, avg_sentence_length, avg_word_length\n",
    "\n",
    "import string \n",
    "import re \n",
    "\n",
    "def compute_text_statistics(text):\n",
    "    # Tokenize: remove punctuation, lowercase\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    tokens = text.lower().translate(translator).split()\n",
    "    \n",
    "    # Vocabulary richness\n",
    "    vocab_richness = len(set(tokens)) / len(tokens) if tokens else 0\n",
    "    \n",
    "    # Sentences: split on sentence-ending punctuation\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s for s in sentences if s.strip()]\n",
    "    \n",
    "    # Average sentence length\n",
    "    sentence_lengths = []\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = sentence.lower().translate(translator).split()\n",
    "        sentence_lengths.append(len(sentence_tokens))\n",
    "    avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0\n",
    "    \n",
    "    # Average word length\n",
    "    word_length = [len(word) for word in tokens]\n",
    "    avg_word_length = sum(word_length) / len(word_length) if word_length else 0\n",
    "    \n",
    "    return {\n",
    "        'vocab_richness': vocab_richness,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_word_length': avg_word_length\n",
    "    }\n",
    "\n",
    "stats = compute_text_statistics(story_text)\n",
    "print(\"Statistics for 'The Red-Headed League':\")\n",
    "print(f\"Vocabulary Richness: {stats['vocab_richness']:.2f}\")\n",
    "print(f\"Average Sentence Length: {stats['avg_sentence_length']:.2f} words\")\n",
    "print(f\"Average Word Length: {stats['avg_word_length']:.2f} characters\")\n",
    "\n",
    "#  The vocabulary richness is rather low and and the average sentence length and average word length is rather low compared to modern text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5545405",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2: Main Characters\n",
    "\n",
    "> \"Who are the main characters in this story?\"\n",
    "\n",
    "**NLP Method:** Use Named Entity Recognition (NER) to extract PERSON entities.\n",
    "\n",
    "**Hint:** Use spaCy's `en_core_web_sm` model. Process the text and filter entities where `ent.label_ == 'PERSON'`. Count how often each name appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "332a59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: extract PERSON entities using spaCy NER\n",
    "# You'll need: import spacy, nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/characters.txt\", \"w\") as f:\n",
    "#     for name in your_character_list:\n",
    "#         f.write(f\"{name}\\n\")\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(story_text)\n",
    "raw_names = sorted(set(ent.text for ent in doc.ents if ent.label_ == \"PERSON\"))\n",
    "clean_names = [name.replace(\"'s\", \"\") for name in raw_names]\n",
    "clean_names = [name for name in clean_names if len(name.split()) > 1]\n",
    "\n",
    "remove = {\n",
    "    \"Agra\", \"Bosombe Pool\", \"Pall Mall\", \"Waterloo\", \"Waterloo Bridge\", \"St.\", \"XI\", \"Square\", \"Surrey\", \"Near Lee\", \"Principal\", \"F.H.M ' Now\", \"I.\", \"I. '\"\n",
    "}\n",
    "\n",
    "clean_names = [name for name in clean_names if name not in remove]\n",
    "\n",
    "merge_map = {\n",
    "    \"Holmes\": \"Sherlock Holmes\",\n",
    "    \"Sherlock\": \"Sherlock Holmes\",\n",
    "    \"Watson\": \"Dr. Watson\"\n",
    "   \n",
    "}\n",
    "final_names = sorted(set(merge_map.get(name, name) for name in clean_names))\n",
    "\n",
    "with open(\"output/characters.txt\", \"w\") as f:\n",
    "    for name in final_names:\n",
    "        f.write(f\"{name}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732e661",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3: Story Locations\n",
    "\n",
    "> \"Where does the story take place?\"\n",
    "\n",
    "**NLP Method:** Use Named Entity Recognition (NER) to extract location entities (GPE and LOC).\n",
    "\n",
    "**Hint:** Filter entities where `ent.label_` is 'GPE' (geopolitical entity) or 'LOC' (location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3f8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: extract GPE and LOC entities using spaCy NER\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/locations.txt\", \"w\") as f:\n",
    "#     for place in your_locations_list:\n",
    "#         f.write(f\"{place}\\n\")\n",
    "\n",
    "doc = nlp(story_text)\n",
    "your_locations_list = [ent.text for ent in doc.ents if ent.label_ in {\"GPE\", \"LOC\"}]\n",
    "your_locations_list = sorted(list(set(your_locations_list)))\n",
    "your_locations_list = [name.replace(\"'s\", \"\") for name in your_locations_list]\n",
    "\n",
    "remove = {\n",
    "    \"Holmes\", \"Horace\", \"Rucastle\", \"Uffa\", \"geese\", \"crisply\", \"n't\", \"the\", \"Street\", \"wooden\", \"Scarlet\", \"Esq\", \"Major Prendergast\", \"the West End\", \"the City\", \"the City and Suburban Bank\", \"the St. Pancras Hotel\", \"the Amoy River\", \"8_s\", \"Cal.\", \"D.D.\", \"Pa.\", \"the\", \"lodgings\", \"City\", \"Union\", \"Hotel\", \"west.\", \"china\", \"morocco\"\n",
    "}\n",
    "\n",
    "generic_terms = {\"North\", \"South\", \"East\", \"West\", \"Captial\", \"City\", \"States\", \"Union\"}\n",
    "\n",
    "your_locations_list = [name for name in your_locations_list if name not in remove and name not in generic_terms]\n",
    "\n",
    "\"\"\n",
    "with open(\"output/locations.txt\", \"w\") as f:\n",
    "    for place in your_locations_list:\n",
    "        f.write(f\"{place}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b228d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4: Wilson's Business\n",
    "\n",
    "> \"What is Wilson's business?\"\n",
    "\n",
    "**NLP Method:** Use TF-IDF similarity to find which section discusses Wilson's business.\n",
    "\n",
    "**Hint:** Create a TF-IDF vectorizer, fit it on the 3 sections, then transform your query using the same vectorizer (`.transform()`, not `.fit_transform()` - you want to use the vocabulary learned from the sections). Find which section has the highest cosine similarity and read it to find the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24f7fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: use TF-IDF similarity to find the relevant section\n",
    "# You'll need: from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#              from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/business.txt\", \"w\") as f:\n",
    "#     f.write(\"Wilson's business is: ...\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query = \"Wilson's business\"\n",
    "\n",
    "sections = story_text.split(\"\\n\\n\")  \n",
    "sections = [s.replace(\"\\n\", \" \") for s in sections if s.strip()]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(sections + [query])\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "most_relevant_index = cosine_similarities.argmax()\n",
    "most_relevant_section = sections[most_relevant_index]\n",
    "\n",
    "with open(\"output/business.txt\", \"w\") as f:\n",
    "    f.write(f\"Wilson's business is:\\n\\n{most_relevant_section}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85452bf1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 5: Wilson's Work Routine\n",
    "\n",
    "> \"What is Wilson's daily work routine for the League?\"\n",
    "\n",
    "**NLP Method:** Use TF-IDF similarity to find which section discusses Wilson's work routine.\n",
    "\n",
    "**Hint:** Similar to Question 4 - use TF-IDF to find the section that best matches your query about work routine. The answer includes what Wilson had to do and what eventually happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ddea14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: use TF-IDF similarity to find the relevant section\n",
    "\n",
    "# When done, save your findings:\n",
    "# with open(\"output/routine.txt\", \"w\") as f:\n",
    "#     f.write(\"Wilson's work routine: ...\\n\")\n",
    "#     f.write(\"What happened: ...\\n\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "paragraphs = [p.strip() for p in story_text.split(\"\\n\\n\") if len(p.strip()) > 50]\n",
    "\n",
    "query = \"Wilson's work routine\"\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words ='english')\n",
    "tfidf = vectorizer.fit_transform([query] + paragraphs)\n",
    "\n",
    "similarities = cosine_similarity(tfidf[0:1], tfidf[1:]).flatten()\n",
    "\n",
    "best_idx = similarities.argmax()\n",
    "best_paragraph = paragraphs[best_idx]\n",
    "\n",
    "what_happened = \"\"\n",
    "if best_idx + 1 < len(paragraphs):\n",
    "    what_happened = paragraphs[best_idx + 1]\n",
    "\n",
    "with open(\"output/routine.txt\", \"w\") as f:\n",
    "    f.write(f\"Wilson's work routine:\\n{best_paragraph}\\n\\n\")\n",
    "    f.write(f\"What happened:\\n{what_happened}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
